{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as w,stopwords \n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading word2vec model\n",
    "model=gensim.models.Word2Vec.load('../Word2Vec_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetching aspects\n",
    "aspects={}\n",
    "for root, dirs, files in os.walk('../Aspects/'):\n",
    "        for name in files:\n",
    "            with open('../Aspects/'+name) as f:\n",
    "                aspects[name]=json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialising porter stemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetch all reviews\n",
    "data=[]\n",
    "for root, dirs, files_review in os.walk('../Reviews/'):\n",
    "    for name in dirs:                             #all review folders\n",
    "        with open('../Reviews/'+name+'/review.txt') as f:\n",
    "            data=data+json.load(f)\n",
    "            f.close()\n",
    "dataset=[]\n",
    "dataset=np.asarray(data)\n",
    "dataset=np.concatenate(dataset)               #contains all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):                                # for removing stopwords\n",
    "    stop=set(stopwords.words('english'))\n",
    "    stopwords_tokens=[token for token in tokens if token not in stop]\n",
    "    return stopwords_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyse_aspect(tokens):            # analyses to which aspect a set of tokens of a sentence belong to\n",
    "    aspect_flags={}\n",
    "    for aspect_name in files:\n",
    "        aspect_flags[aspect_name]=0\n",
    "    for token in tokens:\n",
    "        temp_token=ps.stem(token)\n",
    "        for aspect_name in files:\n",
    "            for aspect in aspects[aspect_name]:\n",
    "                temp_aspect=ps.stem(aspect)\n",
    "                if(temp_token == temp_aspect):\n",
    "                    aspect_flags[aspect_name]=aspect_flags[aspect_name]+1\n",
    "    return aspect_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_sentiment(aspect_flags,sentence):    #analyses the sentiment and stores the sentence to the specified class\n",
    "    temp=TextBlob(sentence)\n",
    "    polarity=temp.sentiment[0]            \n",
    "    if(polarity>=0 and polarity<=0.25):\n",
    "        file_name='neg'\n",
    "    elif(polarity>0.25 and polarity<=0.45):\n",
    "        file_name='sli_neg'\n",
    "    elif(polarity>0.55 and polarity<=0.75):\n",
    "        file_name='sli_pos'\n",
    "    elif(polarity>0.75 and polarity<=1):\n",
    "        file_name='pos' \n",
    "    else :\n",
    "        file_name='neutral'\n",
    "    #print 'sentence :'+ sentence\n",
    "    #print 'polarity :'+ file_name\n",
    "    \n",
    "    gen_flag=0\n",
    "    for aspect_name in files:                 # for all aspect names\n",
    "        if(aspect_flags[aspect_name]>0):\n",
    "            #print 'aspect name :' +aspect_name\n",
    "            gen_flag=1             # for general category\n",
    "            file_ptr=open('../Dataset/'+aspect_name+'/'+file_name,'r')\n",
    "            temp_data=json.load(file_ptr)\n",
    "            file_ptr.close()\n",
    "            file_ptr=open('../Dataset/'+aspect_name+'/'+file_name,'w')\n",
    "            temp_data.append(sentence)\n",
    "            json.dump(temp_data,file_ptr)\n",
    "            file_ptr.close() \n",
    "    \n",
    "    if(gen_flag==0):\n",
    "        #print 'Aspect Name : general category'\n",
    "        file_ptr=open('../Dataset/Aspect7/'+file_name,'r')\n",
    "        temp_data=json.load(file_ptr)\n",
    "        file_ptr.close()\n",
    "        file_ptr=open('../Dataset/Aspect7/'+file_name,'w')\n",
    "        temp_data.append(sentence)\n",
    "        json.dump(temp_data,file_ptr)\n",
    "        file_ptr.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokenize_sent(sentence):                         # for tokenizing single sentence\n",
    "    temp_tokens=nltk.word_tokenize(sentence)\n",
    "    sent_tokens=remove_stopwords(temp_tokens)         # to be reconsidered\n",
    "    aspect_flags=analyse_aspect(sent_tokens)\n",
    "    analyse_sentiment(aspect_flags,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_review(review):                # for tokenizing single reviews\n",
    "    review=review.lower()\n",
    "    review=TextBlob(review)\n",
    "    review=str(review.correct())           #to be corrected\n",
    "    review=review.replace('.',',')        \n",
    "    sentences=re.split(' and | but |, ',review)        #splitting criteria\n",
    "    for sentence in sentences:\n",
    "        tokenize_sent(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_all(dataset):               # for tokenizing all the reviews\n",
    "    for review in dataset:\n",
    "        print review\n",
    "        tokenize_review(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c5d255c922b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenize_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m70000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-dd6fac9b7d72>\u001b[0m in \u001b[0;36mtokenize_all\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtokenize_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-49e686a40649>\u001b[0m in \u001b[0;36mtokenize_review\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' and | but |, '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#splitting criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtokenize_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-837b6fec97bc>\u001b[0m in \u001b[0;36mtokenize_sent\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msent_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_tokens\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# to be reconsidered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maspect_flags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalyse_aspect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0manalyse_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspect_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-7940b703f9f6>\u001b[0m in \u001b[0;36manalyse_sentiment\u001b[0;34m(aspect_flags, sentence)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtemp_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mfile_ptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mfile_ptr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Dataset/Aspect7/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mtemp_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenize_all(dataset[70000:70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clear all files\n",
    "def clear_all():\n",
    "    temp_files=list(files)\n",
    "    temp_files.append('Aspect7')\n",
    "    for file_name in files:\n",
    "        with open('../Dataset/'+file_name+'/pos','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/neg','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/sli_pos','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/sli_neg','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/neutral','w') as f:\n",
    "            json.dump([],f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clear_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf8'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
