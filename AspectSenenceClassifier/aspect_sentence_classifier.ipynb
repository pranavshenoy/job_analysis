{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named Counter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7cc361945307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named Counter"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as w,stopwords \n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading word2vec model\n",
    "model=gensim.models.Word2Vec.load('../Word2Vec_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetching aspects\n",
    "aspects={}\n",
    "for root, dirs, files in os.walk('../Aspects/'):\n",
    "        for name in files:\n",
    "            with open('../Aspects/'+name) as f:\n",
    "                aspects[name]=json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialising porter stemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetch all reviews\n",
    "data=[]\n",
    "for root, dirs, files_review in os.walk('../Reviews/'):\n",
    "    for name in dirs:                             #all review folders\n",
    "        with open('../Reviews/'+name+'/review.txt') as f:\n",
    "            data=data+json.load(f)\n",
    "            f.close()\n",
    "dataset=[]\n",
    "dataset=np.asarray(data)\n",
    "dataset=np.concatenate(dataset)               #contains all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):                                # for removing stopwords\n",
    "    stop=set(stopwords.words('english'))\n",
    "    stopwords_tokens=[token for token in tokens if token not in stop]\n",
    "    return stopwords_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyse_aspect(tokens):            # analyses to which aspect a set of tokens of a sentence belong to\n",
    "    aspect_flags={}\n",
    "    for aspect_name in files:\n",
    "        aspect_flags[aspect_name]=0\n",
    "    for token in tokens:\n",
    "        temp_token=ps.stem(token)\n",
    "        for aspect_name in files:\n",
    "            for aspect in aspects[aspect_name]:\n",
    "                temp_aspect=ps.stem(aspect)\n",
    "                if(temp_token == temp_aspect):\n",
    "                    aspect_flags[aspect_name]=aspect_flags[aspect_name]+1\n",
    "    return aspect_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_sentiment(aspect_flags,sentence):    #analyses the sentiment and stores the sentence to the specified class\n",
    "    temp=TextBlob(sentence)\n",
    "    polarity=temp.sentiment[0]            \n",
    "    if(polarity>=-1 and polarity<=-0.5):\n",
    "        file_name='neg'\n",
    "    elif(polarity>-0.5 and polarity<=-0.1):\n",
    "        file_name='sli_neg'\n",
    "    elif(polarity>0.1 and polarity<=0.5):\n",
    "        file_name='sli_pos'\n",
    "    elif(polarity>0.5 and polarity<=1):\n",
    "        file_name='pos' \n",
    "    else :\n",
    "        file_name='neutral'\n",
    "    #print 'sentence :'+ sentence\n",
    "    #print 'polarity :'+ file_name\n",
    "    \n",
    "    gen_flag=0\n",
    "    for aspect_name in files:                 # for all aspect names\n",
    "        if(aspect_flags[aspect_name]>0):\n",
    "            #print 'aspect name :' +aspect_name\n",
    "            gen_flag=1             # for general category\n",
    "            file_ptr=open('../Dataset/'+aspect_name+'/'+file_name,'r')\n",
    "            temp_data=json.load(file_ptr)\n",
    "            file_ptr.close()\n",
    "            file_ptr=open('../Dataset/'+aspect_name+'/'+file_name,'w')\n",
    "            temp_data.append(sentence)\n",
    "            json.dump(temp_data,file_ptr)\n",
    "            file_ptr.close() \n",
    "    \n",
    "    if(gen_flag==0):\n",
    "        #print 'Aspect Name : general category'\n",
    "        file_ptr=open('../Dataset/Aspect7/'+file_name,'r')\n",
    "        temp_data=json.load(file_ptr)\n",
    "        file_ptr.close()\n",
    "        file_ptr=open('../Dataset/Aspect7/'+file_name,'w')\n",
    "        temp_data.append(sentence)\n",
    "        json.dump(temp_data,file_ptr)\n",
    "        file_ptr.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokenize_sent(sentence):                         # for tokenizing single sentence\n",
    "    temp_tokens=nltk.word_tokenize(sentence)\n",
    "    sent_tokens=remove_stopwords(temp_tokens)         # to be reconsidered\n",
    "    aspect_flags=analyse_aspect(sent_tokens)\n",
    "    analyse_sentiment(aspect_flags,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_review(review):                # for tokenizing single reviews\n",
    "    review=review.lower()\n",
    "    review=TextBlob(review)\n",
    "    review=str(review.correct())           #to be corrected\n",
    "    review=review.replace('.',',')        \n",
    "    sentences=re.split(' and | but |, ',review)        #splitting criteria\n",
    "    for sentence in sentences:\n",
    "        tokenize_sent(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_all(dataset):               # for tokenizing all the reviews\n",
    "    for review in dataset:\n",
    "        print review\n",
    "        tokenize_review(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenize_all(dataset[10000:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clear all files\n",
    "def clear_all():\n",
    "    temp_files=list(files)\n",
    "    temp_files.append('Aspect7')\n",
    "    for file_name in temp_files:\n",
    "        with open('../Dataset/'+file_name+'/pos','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/neg','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/sli_pos','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/sli_neg','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/neutral','w') as f:\n",
    "            json.dump([],f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clear_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf8'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
