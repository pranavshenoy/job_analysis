{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as w,stopwords \n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading word2vec model\n",
    "model=gensim.models.Word2Vec.load('../Word2Vec_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetching aspects\n",
    "aspects={}\n",
    "for root, dirs, files in os.walk('../Aspects/'):\n",
    "        for name in files:\n",
    "            with open('../Aspects/'+name) as f:\n",
    "                aspects[name]=json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialising porter stemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetch all reviews\n",
    "data=[]\n",
    "for root, dirs, files_review in os.walk('../Reviews/'):\n",
    "    for name in dirs:                             #all review folders\n",
    "        with open('../Reviews/'+name+'/review.txt') as f:\n",
    "            data=data+json.load(f)\n",
    "            f.close()\n",
    "dataset=[]\n",
    "dataset=np.asarray(data)\n",
    "dataset=np.concatenate(dataset)               #contains all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):                                # for removing stopwords\n",
    "    stop=set(stopwords.words('english'))\n",
    "    stopwords_tokens=[token for token in tokens if token not in stop]\n",
    "    return stopwords_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyse_aspect(tokens):            # analyses to which aspect a set of tokens of a sentence belong to\n",
    "    aspect_flags={}\n",
    "    for aspect_name in files:\n",
    "        aspect_flags[aspect_name]=0\n",
    "    for token in tokens:\n",
    "        temp_token=ps.stem(token)\n",
    "        for aspect_name in files:\n",
    "            for aspect in aspects[aspect_name]:\n",
    "                temp_aspect=ps.stem(aspect)\n",
    "                if(temp_token == temp_aspect):\n",
    "                    aspect_flags[aspect_name]=aspect_flags[aspect_name]+1\n",
    "    return aspect_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_sentiment(aspect_flags,sentence):    #analyses the sentiment and stores the sentence to the specified class\n",
    "    temp=TextBlob(sentence)\n",
    "    polarity=temp.sentiment[0]            \n",
    "    if(polarity>=-1 and polarity<=-0.5):\n",
    "        file_name='neg'\n",
    "    elif(polarity>-0.5 and polarity<=-0.1):\n",
    "        file_name='sli_neg'\n",
    "    elif(polarity>0.1 and polarity<=0.5):\n",
    "        file_name='sli_pos'\n",
    "    elif(polarity>0.5 and polarity<=1):\n",
    "        file_name='pos' \n",
    "    else :\n",
    "        file_name='neutral'\n",
    "    #print 'sentence :'+ sentence\n",
    "    #print 'polarity :'+ file_name\n",
    "    \n",
    "    gen_flag=0\n",
    "    for aspect_name in files:                 # for all aspect names\n",
    "        if(aspect_flags[aspect_name]>0):\n",
    "            #print 'aspect name :' +aspect_name\n",
    "            gen_flag=1             # for general category\n",
    "            file_ptr=open('../Dataset/'+aspect_name+'/'+file_name,'r')\n",
    "            temp_data=json.load(file_ptr)\n",
    "            file_ptr.close()\n",
    "            file_ptr=open('../Dataset/'+aspect_name+'/'+file_name,'w')\n",
    "            temp_data.append(sentence)\n",
    "            json.dump(temp_data,file_ptr)\n",
    "            file_ptr.close() \n",
    "    \n",
    "    if(gen_flag==0):\n",
    "        #print 'Aspect Name : general category'\n",
    "        file_ptr=open('../Dataset/Aspect7/'+file_name,'r')\n",
    "        temp_data=json.load(file_ptr)\n",
    "        file_ptr.close()\n",
    "        file_ptr=open('../Dataset/Aspect7/'+file_name,'w')\n",
    "        temp_data.append(sentence)\n",
    "        json.dump(temp_data,file_ptr)\n",
    "        file_ptr.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokenize_sent(sentence):                         # for tokenizing single sentence\n",
    "    temp_tokens=nltk.word_tokenize(sentence)\n",
    "    sent_tokens=remove_stopwords(temp_tokens)         # to be reconsidered\n",
    "    aspect_flags=analyse_aspect(sent_tokens)\n",
    "    analyse_sentiment(aspect_flags,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_review(review):                # for tokenizing single reviews\n",
    "    review=review.lower()\n",
    "    review=TextBlob(review)\n",
    "    review=str(review.correct())           #to be corrected\n",
    "    review=review.replace('.',',')        \n",
    "    sentences=re.split(' and | but |, ',review)        #splitting criteria\n",
    "    for sentence in sentences:\n",
    "        tokenize_sent(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_all(dataset):               # for tokenizing all the reviews\n",
    "    count=Counter();\n",
    "    for review in dataset:\n",
    "        count['c'] +=1\n",
    "        print count['c']\n",
    "        tokenize_review(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#default encoding to utf8 \n",
    "import sys\n",
    "sys.getdefaultencoding()\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenize_all(dataset[50005:75000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clear all files\n",
    "def clear_all():\n",
    "    temp_files=list(files)\n",
    "    temp_files.append('Aspect7')\n",
    "    for file_name in temp_files:\n",
    "        with open('../Dataset/'+file_name+'/pos','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/neg','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/sli_pos','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/sli_neg','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/neutral','w') as f:\n",
    "            json.dump([],f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clear_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_all():                                     #to test whether all files are in json format \n",
    "    temp_files=list(files)\n",
    "    temp_files.append('Aspect7')\n",
    "    for file_name in temp_files:\n",
    "        print file_name\n",
    "        with open('../Dataset/'+file_name+'/pos') as f:\n",
    "            json.load(f)\n",
    "        print 'pos  success'    \n",
    "        with open('../Dataset/'+file_name+'/neg') as f:\n",
    "            json.load(f)\n",
    "        print 'neg  success'    \n",
    "        with open('../Dataset/'+file_name+'/sli_pos') as f:\n",
    "            json.load(f)\n",
    "        print 'sli_pos  success'    \n",
    "        with open('../Dataset/'+file_name+'/sli_neg') as f:\n",
    "            json.load(f)\n",
    "        print 'sli_neg  success'    \n",
    "        with open('../Dataset/'+file_name+'/neutral') as f:\n",
    "            json.load(f)   \n",
    "        print 'neutral  success'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
