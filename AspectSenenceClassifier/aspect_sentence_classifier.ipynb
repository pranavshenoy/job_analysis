{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as w,stopwords \n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import enchant\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialising dictionary, tokenizer, stemmer\n",
    "dictionary = enchant.Dict(\"en_US\")\n",
    "tokenizer=RegexpTokenizer('[a-zA-Z]+')\n",
    "ps=PorterStemmer()\n",
    "all_stopwords=stopwords.words('english')+['ism','amazon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading word2vec model\n",
    "model=gensim.models.Word2Vec.load('../Word2Vec_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'compensation', 0.8923154473304749),\n",
       " (u'hike', 0.7944278717041016),\n",
       " (u'pay', 0.783841609954834),\n",
       " (u'wages', 0.7133928537368774),\n",
       " (u'hikes', 0.7015707492828369),\n",
       " (u'salaries', 0.6739672422409058),\n",
       " (u'wage', 0.6642293930053711),\n",
       " (u'remuneration', 0.6438825726509094),\n",
       " (u'increment', 0.6369696855545044),\n",
       " (u'raises', 0.6115041971206665)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# storing similar words\n",
    "aspect_pool=[]\n",
    "for aspect_name in files:\n",
    "    aspect_pool=list(aspects[aspect_name])\n",
    "    for aspect in aspects[aspect_name]:\n",
    "        extra_aspect=[word for word,polarity in model.most_similar(aspect)]\n",
    "        aspect_pool.extend(extra_aspect)\n",
    "    aspect_pool=list(set(aspect_pool))\n",
    "    #with open('../Aspects/'+aspect_name,'w') as f:\n",
    "    #    json.dump(aspect_pool,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetching aspects\n",
    "aspects={}\n",
    "for root, dirs, files in os.walk('../Aspects/'):\n",
    "        for name in files:\n",
    "            with open('../Aspects/'+name) as f:\n",
    "                aspects[name]=json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetch all reviews\n",
    "data=[]\n",
    "for root, dirs, files_review in os.walk('../Reviews/'):\n",
    "    for name in dirs:                             #all review folders\n",
    "        with open('../Reviews/'+name+'/review.txt') as f:\n",
    "            data=data+json.load(f)\n",
    "            f.close()\n",
    "dataset=[]\n",
    "dataset=np.asarray(data)\n",
    "dataset=np.concatenate(dataset)               #contains all the reviews    #converted to a single long array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86989,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total reviews\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyse_aspect(tokens):            # analyses to which aspect a set of tokens of a sentence belong to\n",
    "    aspect_flags={}\n",
    "    for aspect_name in files:\n",
    "        aspect_flags[aspect_name]=0\n",
    "    for token in tokens:\n",
    "        #temp_token=ps.stem(token)\n",
    "        temp_token=token        \n",
    "        for aspect_name in files:\n",
    "            for aspect in aspects[aspect_name]:\n",
    "                temp_aspect=ps.stem(aspect)\n",
    "                if(temp_token == temp_aspect):\n",
    "                    aspect_flags[aspect_name]=aspect_flags[aspect_name]+1\n",
    "    return aspect_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_sentiment(aspect_flags,sentence):    #analyses the sentiment and stores the sentence to the specified class\n",
    "    temp=TextBlob(sentence)\n",
    "    polarity=temp.sentiment[0]            \n",
    "    if(polarity>=-1 and polarity<=-0.5):\n",
    "        file_name='neg'\n",
    "    elif(polarity>-0.5 and polarity<=-0.1):\n",
    "        file_name='sli_neg'\n",
    "    elif(polarity>0.1 and polarity<=0.5):\n",
    "        file_name='sli_pos'\n",
    "    elif(polarity>0.5 and polarity<=1):\n",
    "        file_name='pos' \n",
    "    else :\n",
    "        file_name='neutral'\n",
    "    #print 'sentence :'+ sentence\n",
    "    #print 'polarity :'+ file_name\n",
    "    \n",
    "    gen_flag=0\n",
    "    for aspect_name in files:                 # for all aspect names\n",
    "        if(aspect_flags[aspect_name]>0):\n",
    "            #print 'aspect name :' +aspect_name\n",
    "            gen_flag=1             # for general category\n",
    "            # load data, append sentence and store it in file\n",
    "            file_ptr=open('../Dataset_new/'+aspect_name+'/'+file_name,'r')     \n",
    "            temp_data=json.load(file_ptr)\n",
    "            file_ptr.close()\n",
    "            file_ptr=open('../Dataset_new/'+aspect_name+'/'+file_name,'w')\n",
    "            temp_data.append(sentence)\n",
    "            json.dump(temp_data,file_ptr)\n",
    "            file_ptr.close() \n",
    "    \n",
    "    if(gen_flag==0):\n",
    "        #print 'Aspect Name : general category'\n",
    "        file_ptr=open('../Dataset_new/Aspect7/'+file_name,'r')\n",
    "        temp_data=json.load(file_ptr)\n",
    "        file_ptr.close()\n",
    "        file_ptr=open('../Dataset_new/Aspect7/'+file_name,'w')\n",
    "        temp_data.append(sentence)\n",
    "        json.dump(temp_data,file_ptr)\n",
    "        file_ptr.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    dataset=[]\n",
    "    #print 'sentence : '+sentence\n",
    "    temp=tokenizer.tokenize(sentence)\n",
    "    #print 'tokenizer : '+str(temp)\n",
    "    remove_stopword=[word for word in temp if word not in all_stopwords]    # removing stopwords\n",
    "    #print 'stopwords : '+ str(remove_stopword)\n",
    "    only_english=[word for word in remove_stopword if dictionary.check(word)== True]  # remove non-english words\n",
    "    #print 'removed all non-english words : '+str(only_english)\n",
    "    return  ' '.join(only_english)  # returns a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : this is a  good company having great infrastructure\n",
      "tokenizer : ['this', 'is', 'a', 'good', 'company', 'having', 'great', 'infrastructure']\n",
      "stopwords : ['good', 'company', 'great', 'infrastructure']\n",
      "removed all non-english words : ['good', u'compani', 'great', u'infrastructur']\n",
      "Final Sentence : good compani great infrastructur\n",
      " \n",
      "Aspects : {'Aspect6': 0, 'Aspect5': 1, 'Aspect4': 0, 'Aspect3': 0, 'Aspect2': 0, 'Aspect1': 0}\n",
      " \n",
      "Sentiment Value : Sentiment(polarity=0.75, subjectivity=0.675)\n"
     ]
    }
   ],
   "source": [
    "#only for displaying\n",
    "sample=preprocess('this is a  good company having great infrastructure')\n",
    "print 'Final Sentence : '+str(sample)\n",
    "print ' '\n",
    "sample_aspect_flag=analyse_aspect(sample.split())\n",
    "print 'Aspects : '+str(sample_aspect_flag)\n",
    "print ' '\n",
    "sample_tb=TextBlob(sample)\n",
    "print 'Sentiment Value : '+str(sample_tb.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokenize_sent(sentence):                         # for tokenizing single sentence\n",
    "    sentence=preprocess(sentence)\n",
    "    if len(sentence.split())>1:\n",
    "        aspect_flags=analyse_aspect(sentence.split())\n",
    "        analyse_sentiment(aspect_flags,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_review(review):                # for tokenizing single reviews\n",
    "    review=review.lower()\n",
    "    review=review.replace('.',',')        \n",
    "    sentences=re.split(' and | but |, ',review)        #splitting criteria\n",
    "    for sentence in sentences:\n",
    "        tokenize_sent(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_all(dataset):               # for tokenizing all the reviews\n",
    "    count=Counter();\n",
    "    for review in dataset:\n",
    "        count['c'] +=1\n",
    "        print count['c']\n",
    "        tokenize_review(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#default encoding to utf8 \n",
    "import sys\n",
    "sys.getdefaultencoding()\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenize_all(dataset[:25000])    #executed as batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize_all(dataset[25000:50000])    #executed as batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clear all files\n",
    "def clear_all():\n",
    "    temp_files=list(files)\n",
    "    temp_files.append('Aspect7')\n",
    "    for file_name in temp_files:\n",
    "        with open('../Dataset/'+file_name+'/pos','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/neg','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/sli_pos','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/sli_neg','w') as f:\n",
    "            json.dump([],f)\n",
    "        with open('../Dataset/'+file_name+'/neutral','w') as f:\n",
    "            json.dump([],f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clear_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_all():                                     #to test whether all files are in json format \n",
    "    temp_files=list(files)\n",
    "    temp_files.append('Aspect7')\n",
    "    for file_name in temp_files:\n",
    "        print file_name\n",
    "        with open('../Dataset/'+file_name+'/pos') as f:\n",
    "            json.load(f)\n",
    "        print 'pos  success'    \n",
    "        with open('../Dataset/'+file_name+'/neg') as f:\n",
    "            json.load(f)\n",
    "        print 'neg  success'    \n",
    "        with open('../Dataset/'+file_name+'/sli_pos') as f:\n",
    "            json.load(f)\n",
    "        print 'sli_pos  success'    \n",
    "        with open('../Dataset/'+file_name+'/sli_neg') as f:\n",
    "            json.load(f)\n",
    "        print 'sli_neg  success'    \n",
    "        with open('../Dataset/'+file_name+'/neutral') as f:\n",
    "            json.load(f)   \n",
    "        print 'neutral  success'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
