{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from nltk.stem import  PorterStemmer\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():  \n",
    "    dataset=[]\n",
    "    for root, dirs, files in os.walk('../Dataset'):\n",
    "        for directory in dirs:\n",
    "            for root_f, dirs_f, files_f in os.walk(os.path.join(root,directory)):\n",
    "                for each_file in files_f:\n",
    "                    with open(os.path.join(root,directory)+'/'+each_file) as f:\n",
    "                        data_temp=json.load(f)    \n",
    "                    if(each_file == 'pos' or each_file == 'sli_pos'):\n",
    "                        dataset+=[(sentence,'pos') for sentence in data_temp]\n",
    "                    elif(each_file == 'neg' or each_file == 'sli_neg'):\n",
    "                        dataset+=[(sentence,'neg') for sentence in data_temp]\n",
    "                    elif(each_file == 'neutral'):\n",
    "                        dataset+=[(sentence,'neutral') for sentence in data_temp]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_data=load_dataset()    #loading labeled sentences\n",
    "random.shuffle(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tags(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_polarity(word,tag):\n",
    "    #print 'Get Polarity'\n",
    "    #print 'Word : '+str(word)+' Tag : '+str(tag)\n",
    "    synset = swn.senti_synsets(word,tag)\n",
    "    if(len(synset)>0):\n",
    "        pos_pol=synset[0].pos_score()\n",
    "        neg_pol=synset[0].neg_score()\n",
    "        #print str(pos_pol)+' '+str(neg_pol)\n",
    "        #print ' '    \n",
    "        if(pos_pol == neg_pol):\n",
    "            return 0\n",
    "        if(pos_pol>neg_pol):\n",
    "            return pos_pol\n",
    "        else:\n",
    "            return (-neg_pol)\n",
    "    return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frange(start, end, step):\n",
    "    tmp = start\n",
    "    while(tmp < end):\n",
    "        yield tmp\n",
    "        tmp += step  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(sentence):       \n",
    "    features =Counter()\n",
    "    for i in frange(-1,1,0.1):\n",
    "        features[round(i,1)]=0\n",
    "    pos=nltk.pos_tag(sentence.split())\n",
    "    for word,tag in pos:\n",
    "        tag=tags(tag)\n",
    "        polarity=get_polarity(word,tag)\n",
    "        features[round(polarity,1)] += 1\n",
    "        #print features\n",
    "    feature_vector=[]                   # feature vector\n",
    "    for i in frange(-1,1,0.1):\n",
    "        #print i, features[round(i,1)]\n",
    "        feature_vector.append(features[round(i,1)])    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1.0: 0,\n",
       "         -0.9: 0,\n",
       "         -0.8: 0,\n",
       "         -0.7: 0,\n",
       "         -0.6: 0,\n",
       "         -0.5: 0,\n",
       "         -0.4: 0,\n",
       "         -0.3: 0,\n",
       "         -0.2: 0,\n",
       "         -0.1: 0,\n",
       "         -0.0: 1,\n",
       "         0.1: 0,\n",
       "         0.2: 0,\n",
       "         0.3: 0,\n",
       "         0.4: 0,\n",
       "         0.5: 0,\n",
       "         0.6: 0,\n",
       "         0.7: 0,\n",
       "         0.8: 1,\n",
       "         0.9: 0,\n",
       "         1.0: 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features('good learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test = nltk.classify.apply_features(extract_features, labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dividing into training and test data\n",
    "train_set=train_test[:-10000]\n",
    "test_set=train_test[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'price renewal tests high', 'pos')\n",
      "(Counter({-0.0: 2, 0.1: 2, -0.5: 0, 0.6: 0, 0.2: 0, -0.7: 0, 0.4: 0, -0.1: 0, 0.3: 0, -0.9: 0, -0.2: 0, 0.9: 0, -0.6: 0, 1.0: 0, 0.7: 0, -0.3: 0, -0.8: 0, 0.8: 0, -0.4: 0, -1.0: 0, 0.5: 0}), 'pos')\n"
     ]
    }
   ],
   "source": [
    "print labeled_data[5200]\n",
    "print train_set[5200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "with open('senti_naive_bayes_unigram_model','w') as f:\n",
    "    pickle.dump(classifier,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy=nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6052"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
