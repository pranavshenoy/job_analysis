{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from nltk.stem import  PorterStemmer\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset():  \n",
    "    dataset=[]\n",
    "    for root, dirs, files in os.walk('../Dataset'):\n",
    "        for directory in dirs:\n",
    "            for root_f, dirs_f, files_f in os.walk(os.path.join(root,directory)):\n",
    "                for each_file in files_f:\n",
    "                    with open(os.path.join(root,directory)+'/'+each_file) as f:\n",
    "                        data_temp=json.load(f)    \n",
    "                    dataset+=[(sentence,directory) for sentence in data_temp]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialising porterstemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_unigram(data):\n",
    "    dataset=[]\n",
    "    dataset+=[([items for items in sentence.split()],aspect) for sentence,aspect in data]    \n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(x):    #returns all the words in the  data set\n",
    "    vocab = []\n",
    "    for sentence,polarity in x:  \n",
    "        vocab.extend(sentence.split())\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#execution\n",
    "labeled_data=load_dataset()    #loading labeled sentences\n",
    "random.shuffle(labeled_data)\n",
    "\n",
    "vocab=set(get_words(labeled_data)) #getting words \n",
    "vocab=[ps.stem(word) for word in vocab]   #stemming\n",
    "#with open('vocab_unigram','w') as f:\n",
    "#    json.dump(vocab,f)\n",
    "labeled_data=to_unigram(labeled_data)    #converting to unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u'enjoy', u'almost', u'everyth', u'els', u'job', u'pay'], 'Aspect2')\n"
     ]
    }
   ],
   "source": [
    "#bigrams\n",
    "print labeled_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201378\n"
     ]
    }
   ],
   "source": [
    "print len(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6544\n",
      "[u'foul', u'interchang', u'four', u'payoff', u'authorit', u'scold', u'lore', u'lord', u'digit', u'pigment', u'yellow', u'delv', u'politician', u'disturb', u'prize', u'wooden', u'showca', u'habea', u'charter', u'erron', u'nigh', u'rustl', u'miller', u'histor', u'second', u'sooth', u'tether', u'dialogu', u'ruthless', u'thunder', u'specialist', u'aver', u'intellectu', u'crouch', u'avert', u'accu', u'herd', u'gamut', u'china', u'cult', u'deterior', u'militari', u'k', u'unforgett', u'fumbl', u'golden', u'brought', u'sterl', u'stern', u'unit', u'spoke', u'overshadow', u'music', u'telegraph', u'passport', u'strike', u'expiat', u'paperwork', u'relay', u'relax', u'relat', u'notic', u'hurt', u'glass', u'exc', u'holi', u'outskirt', u'midst', u'hold', u'accid', u'blade', u'conceptu', u'sweeter', u'etiquett', u'household', u'virtu', u'cautiou', u'caution', u'want', u'gargantuan', u'shuffl', u'unpaid', u'travel', u'cutback', u'hot', u'hop', u'turk', u'perspect', u'hoy', u'diagram', u'wrong', u'beauti', u'endang', u'warfar', u'revolv', u'dispar', u'someplac', u'wing', u'wind', u'feedback']\n"
     ]
    }
   ],
   "source": [
    "print len(vocab)\n",
    "print vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(document):       #features are bag of words. document is a list of words of a sentence \n",
    "    features = {}\n",
    "    for word in vocab:\n",
    "        features['contains(%s)' % word] = (word in document)\n",
    "    return features        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test = nltk.classify.apply_features(extract_features, labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dividing into training and test data\n",
    "train_set=train_test[:-10000]\n",
    "test_set=train_test[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "#with open('naive_bayes_unigram_model','w') as f:\n",
    "#    pickle.dump(classifier,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#accuracy=nltk.classify.accuracy(classifier, test_set)\n",
    "#with open('accuracy','w') as f:\n",
    "#    json.dump(accuracy,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "with open('naive_bayes_unigram_model') as f:\n",
    "    classifier=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aspect1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample\n",
    "sen='work opportun is bore '\n",
    "classifier.classify(extract_features(sen.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect7: 0.006441\n",
      "Aspect6: 0.232167\n",
      "Aspect5: 0.000175\n",
      "Aspect4: 0.078496\n",
      "Aspect3: 0.024981\n",
      "Aspect2: 0.019896\n",
      "Aspect1: 0.637844\n",
      "0.637843926224\n"
     ]
    }
   ],
   "source": [
    "#calculating the probability\n",
    "dist = classifier.prob_classify(extract_features(sen.split()))\n",
    "polarity=0\n",
    "for label in dist.samples():\n",
    "    print(\"%s: %f\" % (label, dist.prob(label)))\n",
    "    polarity=max(polarity,dist.prob(label))\n",
    "print polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
